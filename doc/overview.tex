\documentclass{article}
\usepackage{listings}

\begin{document}

\section{Purpose}
Chrombpnet-heavy is an expansion of the chrombpnet-lite repository, designed to allow for more flexibility. 
The two main changes are the addition of multi-task models, and a generalization of the regression step. 
The general workflow is the following:

\begin{enumerate}
    \item Prepare bigwig tracks and select the regions you are interested in. 
    \item Train a bias (AKA solo) model. 
    \item Train a transformation model to match the bias model to the experimental data. 
    \item Train a residual model to explain non-bias parts of the experimental data. 
    \item Measure the performance of the full model. 
    \item Make predictions from the full model and residual model.
    \item Generate importance scores from the residual model, either one-dimensionally, as is usual, or by making two-dimensional PISA plots. 
    \item Run MoDISco. 
\end{enumerate}

\section{Philosophy}
These are the guidelines for code design in chrombpnet-heavy:

\begin{itemize}
    \item Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new features.
    \item Expect the output of every program to become the input to another, as yet unknown, program. Don't clutter the output with extraneous information. Don't insist on interactive input. 
    \item Explicit is better than implicit. Wherever possible, do not allow for defaults when a value is not specified. 
    \item Flat is better than nested. Write code that has minimal architectural overhead. 
    \item Errors should never pass silently.
    \item If the implementation is hard to explain, it's a bad idea.
    \item The API should mirror the CLI. 
\end{itemize}


\section{Programs}
chrombpnet-heavy includes the following utilities:
\begin{description}
    \item [trainSoloModel.py] Takes in a bias training input configuration and trains up a seqmodel to predict the bias track. Saves the seqmodel to disk, along with information from the training phase. 
    \item [trainTransformationModel.py] Takes in a bias model and the actual experimental data. Derives a relation to best fit the bias profile onto the experimental data. Saves a new model to disk, adding a simple layer or two to do the regression. 
    \item [trainResidualModel.py] Takes a regressed bias model and experimental data and builds a seqmodel to explain the residuals. Saves the model to disk. 
    \item [makePredictions.py] Takes a trained chrombpnet model and predicts over the given regions. 
    \item [interpret] Generates shap scores of the same type as BPNet - hypothetical contributions for each base are written to a modisco-compatible h5. 
    \item [run-modisco] Runs modisco on the importance score h5 file. 
    \item [metrics] Generates a suite of metrics about how good the model's predictions are. 
    \item [interpret-pisa] Runs an all-to-all shap analysis on the given regions. 
    \item [shap-h5-to-bw] Converts a shap hdf5 file into a bigwig track for visualization. 
\end{description}


\section{Input specification}
In this section, I will provide detailed specifications for all of the input files, with particular emphasis on configuration json files.

\subsection{Solo training input configuration}
A solo training input configuration file is a JSON file that specifies what data should be used to train up the solo model.
It shall contain exactly one regions section, and one input track section for each task that the solo model is being trained on. 

\begin{lstlisting}
<solo-training-input-configuration> ::= 
   {<solo-settings-section> , <beds-section>, 
     <head-section>}

<solo-settings-section> ::= 
    "settings" : {<solo-settings-contents>}

<beds-section> ::= 
    "regions" : [<individual-bed-list>]

<individual-bed-list> ::= 
    <individual-bed> 
  | <individual-bed>, <individual-bed-list>

<individual-bed> ::= 
    {"bed-file" : <file-name>,
     "split" : <splits>,
     <sampling-settings>
     "max-jitter" : <integer>}

<sampling-settings> ::= <empty>
  | "absolute-sampling-weight" : <number>,

<splits> ::= "train"
  |  "val" 
  |  "test"


<head-section> ::= 
  "heads" : [<head-list>]

<head-list> ::= 
    <individual-head>
  | <individual-head>, <head-list>

<individual-head> ::=
    {
      "bigwig-files" : [<bigwig-list>],
      "profile-loss-weight" : <number>,
      "counts-loss-weight" : <number>,
      "head-name" : <string>
    }

<bigwig-list> ::=
    <file-name> 
  | <file-name>, <bigwig-list>

<solo-settings-contents> ::= 
    {"genome" : <file-name>, 
     "output-prefix" : <string>, 
     "epochs" : <integer>, 
     "early-stopping-patience" : <integer>, 
     "batch-size" : <integer>, 
     "learning-rate" : <number>, 
     "architecture" : <solo-architecture-specification> } 

<solo-architecture-specification> ::= 
    {"input-length" : <integer>, 
    "output-length" : <integer>,
    "model-name" : <string>,
    "model-args" : <string>,
    "filters" : <integer>,
    "layers" : <integer>,
    "input-filter-width" : <integer>,
    "output-filter-width" : <integer>}
\end{lstlisting}
                    

\subsection{Transformation input configuration}
The transformation input file is a JSON file that names a solo model and gives the experimental data that it should be fit to. 

\begin{lstlisting}
<transformation-input-configuration> ::=
    {"settings" : <transformation-settings-section>, 
        <beds-section>, 
        <head-section>}

<transformation-settings-section> ::=
    { "output-prefix" : "<string>", 
      "epochs" : <integer>, 
      "early-stopping-patience" : <integer>, 
      "batch-size" : <integer>, 
      "learning-rate" : <number>, 
      "genome" : <file-name>,
      "solo-model-file" : <file-name>,
      "sequence-input-length" : <integer>,
      "output-length" : <integer>,
      "profile-architecture" : <transformation-architecture-specification>, 
      "counts-architecture" : <transformation-architecture-specification> } 

<transformation-architecture-specification> ::= 
    <simple-transformation-architecture-specification>
  | "name" : "passthrough"

<simple-transformation-architecture-specification> ::= 
    "name" : "simple",
    "types" : [<list-of-simple-transformation-types>]

<list-of-simple-transformation-types> ::= 
    <simple-transformation-type>
  | <simple-transformation-type>, <list-of-simple-transformation-types>

<simple-transformation-type> ::= 
    "linear"
  | "sigmoid"
  | "relu"

\end{lstlisting}

This code does not support using an experimental bias track as the input to the 
transformation - it must be a solo model that uses sequence as input.

\subsection{Combined training input configuration}
The full chrombpnet model, known as a combined model is specified by a JSON file, 
much like the bias file.

\begin{lstlisting}

<combined-training-input-configuration> ::= 
   {<combined-settings-section> , <beds-section>, 
     <combined-head-section>}

<combined-head-section> ::= 
  "heads" : [<combined-head-list>]

<combined-head-list> ::= 
    <combined-individual-head>
  | <combined-individual-head>, <combined-head-list>

<combined-individual-head> ::=
    {
      "bigwig-files" : [<bigwig-list>],
      "profile-loss-weight" : <number>,
      "counts-loss-weight" : <number>,
      "head-name" : <string>,
      "use-bias-counts" : <boolean>
}

<combined-settings-section> ::= 
    "settings" : {<combined-settings-contents>}

<combined-settings-contents> ::= 
    {"genome" : <file-name>, 
     "output-directory" : "<string>",
     "epochs" : <integer>, 
     "early-stopping-patience" : <integer>, 
     "batch-size" : <integer>, 
     "learning-rate" : <number>, 
     "transformation-model" : <transformation-combined-settings>,
     "architecture" : <combined-architecture-specification> } 

<transformation-combined-settings> ::= 
   { "transformation-model-file" : <file-name>, 
     "input-length" : <integer>
  }

<combined-architecture-specification> ::= 
    {"input-length" : <number>, 
    "output-length" : <number>,
    "model-name" : <string>,
    "model-args" : <string>,
    "filters" : <number>,
    "layers" : <number>}
\end{lstlisting}

\subsection{Prediction input configuration}
This is the JSON file given to the prediction script. It names the model to use and the predictions to make. 

\begin{lstlisting}
<prediction-input-configuration>::=
    {<prediction-settings-section>, <prediction-bed-section>}

<prediction-settings-section> ::= "settings" : {
    "output-h5" : <file-name>,
    "genome" : <file-name>,
    "batch-size" : <integer>,
    "heads" : [<list-of-prediction-heads>],
    "architecture" : <prediction-model-settings> }


<list-of-prediction-heads> ::=
    <prediction-head> 
  | <prediction-head>, <list-of-prediction-heads>


<prediction-head> ::= {
    "head-id" : <integer>,
    "num-tasks" : <integer>
}

<prediction-bed-section> ::=
    "bed-file" : <file-name>

<prediction-model-settings> ::={
    "model-file" : <file-name>,
    "input-length" : <integer>,
    "output-length" : <integer> }
\end{lstlisting}

\subsection{Prediction to bigwig configuration}
This is the JSON file that specifies how the hdf5-format file from the prediction tool should be rendered to bigwigs. 

\begin{lstlisting}
<prediction-to-bigwig-configuration} ::= {
    "input-h5" : <file-name>,
    "heads" : [<list-of-prediction-to-bigwig-heads>]
}

<list-of-prediction-to-bigwig-heads> ::=
    <prediction-to-bigwig-head>
  | <prediction-to-bigwig-head>, <list-of-prediction-to-bigwig-heads>

<prediction-to-bigwig-head> ::= {
    "head-id" : <integer>,
    "bigwig-files" : [<bigwig-list>],
}


\end{lstlisting}



\subsection{Interpret PISA configuration}

This is the JSON file given to the PISA interpretation tool. 
\begin{lstlisting}
<pisa-configuration> ::= {
    "genome" : <file-name>,
    "bed-file" : <file-name>,
    "model-file" : <file-name>,
    "input-length" : <integer>,
    "output-length" : <integer>,
    "heads" : <integer>,
    "head-id" : <integer>,
    "task-id" : <integer>,
    "output-h5" : <integer>,
    "num-shuffles" : <integer>
}


\end{lstlisting}



It produces an hdf5 format which is organized as follows:




\subsection{Prepare bed configuration}
This is the JSON file given to the input preparation script. It splits your regions into 
test, train, and validation regions, and optionally applies some filtering. 

\begin{lstlisting}
<prepare-bed-configuration> ::={
    <counts-cutoff-section>
    "bigwigs" : [<list-of-bigwigs>],
    "splits" : {<split-settings>},
    "genome" : <file-name>,
    "output-width" : <integer>,
    "input-width" : <integer>,
    "max-jitter" : <integer>,
    <output-file-name-section>,
    "resize-mode" : <resize-mode>
    }

<resize-mode> ::= 
    "none"
  | "center"
  | "start"
  | "peak"

<output-file-name-section> ::=
    "output-prefix" : "<string>"
  | "output-train" : <file-name>,
    "output-val" : <file-name>,
    "output-test" : <file-name>

<counts-cutoff-section> ::=
    <empty>
  | "counts-window-type" : <counts-window-type>,
    "maximum-counts" : [<list-of-integers>],


<counts-window-type> ::=
    "output"
  | "output+jitter"
  | "input" 
  | "input+jitter"


<list-of-bigwigs> ::= 
    <file-name>, <list-of-bigwigs>
  | <file-name>

<split-settings> ::=
    <split-by-chromosome-settings>
  | <split-by-name-settings>
  | <split-by-bed-settings>

<split-by-chromosome-settings> ::=
    "train-chroms" : [<list-of-strings>],
    "val-chroms" : [<list-of-strings> ],
    "test-chroms" : [<list-of-strings> ],
    "regions" : [<list-of-bed-files>]

<split-by-bed-settings> ::=
    "train-regions" : [<list-of-bed-files>],
    "val-regions" : [<list-of-bed-files>],
    "test-regions" : [<list-of-bed-files>]

<split-by-name-settings> ::=
    "regions" : [<list-of-bed-files>],
    "test-regex" : "<string>",
    "train-regex" : "<string>",
    "val-regex" : "<string>"

<list-of-bed-files> ::=
    <file-name>, <list-of-bed-files>
  | <file-name>

\end{lstlisting}

\section{Model architectures}




\end{document}

\documentclass{article}
\usepackage{listings}
\usepackage[cm]{fullpage}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\hypersetup{
    colorlinks=true,
    linktoc=all
}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    columns=fixed
}


\title{BPReveal documentation}
\author{Charles McAnany, Melanie Weilert}

\begin{document}

\newcommand{\deprecation}[1]{\colorbox{orange}{#1}}
\newcommand{\progref}[1]{\hyperref[prog:#1]{\texttt{#1}}}
\maketitle

\tableofcontents

\newpage

\section{Workflow}

BPReveal is an expansion of the chrombpnet-lite repository, designed to allow
for more flexibility.
It also incorporates a new interpretation tool, called PISA (Pairwise
Interaction Shap Analysis) that lets you view a two-dimensional map of cause
and effect over a region.
The two main changes are the addition of multi-task models, and a
generalization of the regression step.
For cases where you don't have a bias track you want to regress out, here are
the steps:

\begin{enumerate}
    \item Prepare bigwig tracks and select the regions you are interested in.
        There are some utilities for this in BPReveal (\progref{prepareBed}),
        but you are mostly on your own for this stage.
    \item Prepare training data files with \progref{prepareTrainingData}.
        These hdf5 files contain the sequences and experimental profiles for
        all the heads in your model.
    \item Train a solo model with \progref{trainSoloModel}.
    \item Measure the performance of your model with \progref{metrics}.
    \item Make predictions from the model with \progref{makePredictionsBed}.
    \item Generate importance scores, either one-dimensionally, with
        \progref{interpretFlat}, or by making two-dimensional PISA plots with
        \progref{interpretPisa}
    \item Run MoDISco to extract motifs (using \progref{shapToNumpy} the
        external \texttt{tfmodisco-lite} package)
    \item Use the motif mapping tools \progref{motifSeqletCutoffs},
        \progref{motifScan} and \progref{motifAddQuantiles} to map the
        discovered motifs back to the genome.
\end{enumerate}

If you do have strong experimental biases, you will need to regress them out.
In that case, the workflow is the following:

\begin{enumerate}
    \item Prepare bigwig tracks and select the regions you are interested in
        with \progref{prepareBed}.
    \item Prepare a data file containing bias using
        \progref{prepareTrainingData}. The bias regions may be uninteresting
        regions in the genome, or you may train on your regions of interest but
        use an experimental control for the data.
    \item Train a bias (AKA solo) model with \progref{trainSoloModel}.
    \item Train a transformation model to match the bias model to the
        experimental data, using \progref{trainTransformationModel}.
    \item Train a residual model to explain non-bias parts of the experimental
        data, using \progref{trainCombinedModel}.
    \item Measure the performance of the full model with \progref{metrics}.
    \item Make predictions from the full model and residual model with
        \progref{makePredictionsBed}.
    \item Generate importance scores from the residual model with
        \progref{interpretFlat}.
    \item Run MoDISco, using scores generated by \progref{shapToNumpy}.
    \item Map the discovered motifs back to the genome using
        \progref{motifSeqletCutoffs}, \progref{motifScan} and
        \progref{motifAddQuantiles}.
\end{enumerate}

\newpage

\section{Philosophy}

These are the guidelines for code design in BPReveal:

\begin{itemize}
    \item Make each program do one thing well.
        To do a new job, build afresh rather than complicate old programs by
        adding new features.
    \item Expect the output of every program to become the input to another,
        as yet unknown, program.
        Don't clutter the output with extraneous information.
        Don't insist on interactive input.
    \item But do include logging messages that can be enabled for debugging.
    \item Explicit is better than implicit.
        Wherever possible, do not allow for defaults when a value is not
        specified.
        (Looking at you, MoDISco!)
    \item For substantial programs, prefer configuration files over a sea of
        command-line arguments.
        Use JSON for all data that aren't (1.) bed files, (2.) bigwig files,
        (3.) fasta files, or (4.) potentially enormous
        (for large datasets, prefer hdf5.).
    \item Flat is better than nested.
        Write code that has minimal architectural overhead.
    \item Don't be too clever.
        The code should use the standard idioms of the language, even if an
        operation could be completed in fewer characters or slightly more
        efficiently some other way.
    \item But performance matters.
        Write code that works fast enough that people can use it to ask new
        questions.
        Use parallelism with wild abandon.
    \item Errors should never pass silently.
    \item One line of code is ten lines of documentation.
        The documentation consists of in-code comments, clear specifications
        (this document), tutorials, and the reference publication.
    \item If the implementation is hard to explain, it's a bad idea.
    \item Use only well-established and stable dependencies.
        Don't require specific versions of libraries, and only require packages
        that are truly essential.
\end{itemize}

\newpage

\section{Programs}

\subsection{Core programs}

\begin{description}
    \item [\progref{trainSoloModel}] Takes in a training input configuration
        and trains up a model to predict the the given data, with no bias
        correction.
        Saves the model to disk, along with information from the training phase.
    \item [\progref{trainTransformationModel}] Takes in a bias (i.e., solo)
        model and the actual experimental (i.e., biology + bias) data.
        Derives a relation to best fit the bias profile onto the experimental
        data.
        Saves a new model to disk, adding a simple layer or two to do the
        regression.
    \item [\progref{trainCombinedModel}] Takes a transformation model and
        experimental data and builds a model to explain the residuals.
        Saves both a combined model and the residual model alone to disk.
    \item [\progref{prepareTrainingData}] Takes in bed and bigwig files and a
        genome, and generates an hdf5-format file containing the samples used
        for training.
    \item [\progref{makePredictionsBed}] Takes a trained model (solo, combined,
        residual, or even transformation models work) and predicts over the
        given regions.
    \item [\progref{makePredictionsFasta}] Takes a trained model and predicts
        on sequences given as a fasta file.
    \item [\progref{interpretFlat}] Generates shap scores of the same type as
        BPNet.
        Hypothetical contributions for each base are written to a
        modisco-compatible h5.
    \item [\progref{interpretPisa}] Runs an all-to-all shap analysis on the
        given bed regions or fasta sequences.
    \item [\progref{motifSeqletCutoffs}] Loads the output from
        \texttt{modiscolite} and calculates cutoff values to use during motif
        scanning.
    \item [\progref{motifScan}] Scan the genome for patterns of contribution
        scores that match motifs identified by modiscolite.
\end{description}

\subsection{Utilities}

\begin{description}
    \item [\progref{metrics}] Calculates a suite of metrics about how good a
        model's predictions are.
    \item [\progref{predictToBigwig}] Takes the hdf5 file generated by the
        predict step and converts one track from it into a bigwig
        file.\label{prog:predictToBigwig}
    \item [\progref{shapToBigwig}] Converts a shap hdf5 file (from
        \progref{interpretFlat}) into a bigwig track for
        visualization.\label{prog:shapToBigwig}
    \item [\progref{shapToNumpy}] Takes the interpretations from
        \texttt{interpretFlat} and converts them to numpy arrays that can be
        read in by modiscolite.\label{prog:shapToNumpy}
    \item [\progref{lengthCalc}] Given the parameters of a network, like input
        filter width, number of layers \&c., determine the input width or
        output width. \label{prog:lengthCalc}
    \item [\progref{makeLossPlots}] Once you've trained a model, you can run
        this on the history file to get plots of all of the components of the
        loss. \label{prog:makeLossPlots}
    \item [\progref{prepareBed}] Given a set of regions and data tracks, reject
        regions that have too few (or too many) reads, or that have unmappped
        bases in the genome.
    \item [\progref{showModel}] Make a pretty picture of your
        model.\label{prog:showModel}
    \item [\progref{motifAddQuantiles}] Takes the output from
        \progref{motifScan} and adds quantile information for determining how
        good your motif matches were.
    \item [\progref{checkJson}] Take a json file and make sure that it's valid
        input for one of the BPReveal programs. Can also be used to identify
        which BPReveal program a json belongs to.\label{prog:checkJson}
\end{description}

\subsection{Libraries}

\begin{description}
    \item [gaOptimize.py] contains tools for evolving sequences that lead to
        desired profiles.
        It implements a genetic algorithm that supports insertions and
        deletions.
    \item [utils.py] Contains general-use utilities and a high-performance tool
        to generate predictions for many sequences.
    \item [bedUtils.py] Useful functions for manipulating bed files,
        particularly for tiling the genome with regions.
    \item [motifUtils.py] Functions for dealing with motif scanning and modisco
        files.
    \item [interpretUtils.py] Functions for getting interpretation scores.
        Contains a streaming system for calculating pisa and flat importance
        scores.
    \item [schema] A set of JSON schemas that validate the inputs to the
        BPReveal programs. These are used to make sure that incorrect inputs
        trigger errors early, and that those errors are clearer to the user.
\end{description}

\newpage

\section{Setup}

BPReveal requires just a few libraries, and I use conda to manage my
environment.
You can also install these with pip.

\begin{itemize}
    \item \texttt{python} version 3.11 or later. Note that this codebase uses
        features that were introduced in version 3.11, so it will not work
        with 3.10 or earlier.
    \item \texttt{tfmodisco-lite}, only needed if you want to do motif
        identification.
    \item \texttt{pyBigWig}, for reading and writing bigwig files.
    \item \texttt{pysam}, to read in fasta files.
    \item \texttt{tensorflow} (This also installs numpy, scipy, and h5py).
    \item \texttt{tensorflow-probability}, which contains functions needed to
        calculate the loss.
    \item \texttt{pybedtools}, for reading and writing bed files.
    \item \texttt{matplotlib}, for rendering the loss plots
        by \texttt{makeLossPlots}.
    \item \texttt{tqdm}, for progress bars.
    \item \texttt{jsonschema}, for checking the validity of inputs.
\end{itemize}

You can, of course, install any other packages you like.
I'm fond of \texttt{jupyter lab} for doing analysis.
I found that I also needed to install a \texttt{cudatoolkit} package to get
everything compatible, but this may not be necessary on your system.
You should check the TensorFlow website for instructions on how to install
TensorFlow on your system.


\newpage

\section{Breaking changes}


\subsection{BPReveal 5.0.0}
When BPReveal 5.0.0 is released, the following breaking changes will occur:
\begin{itemize}
    \item \texttt{prepareBed} will no longer accept a list of bigwigs without
        head information.
        The program currently spits out warnings when you do this, and these
        will become an error.
        See \ref{dep:oldbigwigs}.
    \item Using an importance hdf5 from before version 4.0.0 will now result
        in an error instead of a deprecation warning.
\end{itemize}

\subsection{BPReveal 4.0.0}
When BPReveal 4.0.0 is released, the following breaking changes will occur:
\begin{itemize}
    \item The chromosome list in the hdf5 files produced by
        \texttt{interpretFlat} currently store chromosome information as
        strings, unlike all other output file formats.
        This will change so that chromosomes are numbered.
        \texttt{shapToBigwig} and the motif scanning utilities now emit
        warnings if they detect an old-style importance hdf5. This will become
        an error in 5.0.0.
    \item The adaptive loss algorithm required me to implement a custom mse
        loss.
        In 3.6, I sneakily called it \texttt{"mse"} so you didn't have to add
        another custom object to scope when you load a new model.
        This loss will be renamed \texttt{"reweightableMse"} and you'll have to
        add it to the custom object scopes when you load a model.
        Since the full new loss includes a tensor that must be created
        beforehand, \texttt{losses.py} will include a dummy version that you
        can use to load, but not train, a model.
        See \ref{sec:countsLossReweighting} for the algorithm.
\end{itemize}

\subsection{BPReveal 3.0}

\subsubsection{BPReveal 3.6.0}
\begin{itemize}
    \item The \texttt{predictToBigwig} script now averages the values in
        overlapping regions instead of taking the leftmost base.
        This may result in small changes in generated bigwigs.
    \item In order to accomodate the adaptive loss algorithm
        (\ref{sec:countsLossReweighting}), some of the layer names in
        transformation models were changed.
        If you were depending on these layer names, I'm curious to know how you
        got yourself in that situation.
\end{itemize}

\subsubsection{BPReveal 3.5.0}
\begin{itemize}
    \item BPReveal now uses Python 3.11, instead of 3.10. Users must re-build
        the \texttt{libjaccard} library for the new Python version.
\end{itemize}

\subsubsection{BPReveal 3.0.0}
\begin{itemize}
    \item You must specify a \texttt{"remove-overlaps"} field in configuration
        files for \progref{prepareBed}.
    \item \texttt{cropdown} layers were removed as an option for transformation
        models.
    \item \begin{sloppypar}The transformation model configuration file calls
        the input length \texttt{input-length} instead of
        \texttt{sequence-input-length}.\end{sloppypar}
\end{itemize}


\section{Programs}

In this section, I will provide detailed specifications for all of the input
files, with particular emphasis on configuration JSON files.

\newpage
\subsection{Prepare training data: \texttt{prepareTrainingData}}\label{prog:prepareTrainingData}

This program reads in a genome file, a list of regions in bed format, and a set
of bigwig files containing profiles that the model will predict.
It generates an hdf5-format file that is used during training.
If you want to train on a custom genome, or you don't have a meaningful genome
for your experiment, you can still provide sequences and profiles by creating
an hdf5 file in the same format as this tool generates.

\subsubsection{Input specification}

\lstinputlisting{bnf/prepareTrainingData.bnf}

\subsubsection{Parameter notes}
\begin{itemize}
    \item \texttt{genome} is the name of the fasta-format file for your
        organism.
    \item \texttt{regions} is the name of the bed file of regions you will
        train on.
        These regions must be \texttt{output-width} in length.
    \item The \texttt{reverse-complement} flag sets whether the data files will
        include reverse-complement augmentation.
        If this is set to \texttt{true} then you must include
        \texttt{revcomp-task-order} in every head section.
    \item The \texttt{revcomp-task-order} is a list specifying which tasks in
        the forward sample should map to the tasks in the reverse sample.
        For example, if the two tasks represent reads on the plus and minus
        strand, then when you create a reverse-complemented training example,
        the minus strand becomes the plus strand, and vice versa.
        So you'd set this parameter to \texttt{[1,0]} to indicate that the data
        for the two tasks should be swapped (in addition to reversed 5' to 3',
        of course).
        If you only have one task in a head, you should set this to
        \texttt{[0]}, to indicate that there is no swapping.
        If you have multiple tasks, say, a task for the left end of a read,
        one for the right end, and one for the middle, then the left and right
        should be swapped and the middle left alone.
        In this case, you'd set \texttt{revcomp-task-order} to \texttt{[1,0,2]}.
        If this parameter is set to \texttt{"auto"}, then it will choose
        \texttt{[1,0]} if there are two strands, \texttt{[0]} if there is only
        one strand, and it will issue an error if there are more strands than
        that.
        \texttt{"auto"} is appropriate for data like ChIP-nexus.
\end{itemize}

\subsubsection{Output specification}

It will generate a file that is organized as follows:

\begin{itemize}
    \item \texttt{head\_N}, where \texttt{N} is an integer from 0 to however
        many heads you have.
        It will have shape
        (num-regions x (output-length + 2*jitter) x num-tasks).
    \item (more \texttt{head\_N} entries)...
    \item \texttt{sequence}, which is the one-hot encoded sequence for each
        corresponding region.
        It will have shape (num-regions x (input-width + 2*jitter) x 4).
\end{itemize}


\newpage


\subsection{Training a solo model: \texttt{trainSoloModel}}\label{prog:trainSoloModel}

This program generates a neural network that maps genomic sequence to
experimental readout.
In cases where you don't need to regress out a bias track, this is the only
training program you'll use.
By and large, you can refer to the BPNet paper for details on how this program
works, the only difference is in the input padding.
In the original BPNet, for an output window of 1 kb, the input sequence was
1 kb long, and if a neuron needed to know about bases outside that window, it
got all zeros.
For image processing, this makes sense, because you can't un-crop an image.
However, for DNA in a genome, you can just expand out the windows and get as
much DNA sequence as you like.
Therefore, BPReveal models require an input length that is larger than output
length, so that the model can use DNA sequence information that is outside of
its output window.

The required input files are two hdf5-format files that contain sequences and
profiles.
One of these is used for training, one for validation during training.
See \progref{prepareTrainingData} for the specification for this file.

The program will produce two outputs, one being a Keras model.
This model will be used later for prediction and interpretation.
The other output records the progress of the model during training, and it is
read in by \progref{makeLossPlots}.


\subsubsection{Input specification}

A solo training input configuration file is a JSON file that specifies what data
should be used to train up the solo model.
It shall contain exactly one settings section, one data section, one heads
section, and one verbosity section.

\lstinputlisting{bnf/trainSoloModel.bnf}

\subsubsection{Parameter notes}

\begin{itemize}
    \item The \texttt{profile-loss-weight} is simply a scalar that the profile
        loss is multiplied by.
        This comes in handy when you're training on two datasets with disparate
        coverage.
        Since the MNLL loss is proportional to the number of reads, a track with
        higher coverage will dominate the loss.
        Instead of calculating this \emph{a priori}, I find it easiest to start
        training the model, look at the loss values, and then pick multipliers
        that will make them about even.
    \item The \texttt{counts-loss-weight} is similar to
        \texttt{profile-loss-weight}, but do keep in mind that you need to set
        it even when you're training a single output head, since the mean
        squared error value of the counts prediction tends to be minuscule
        compared to the MNLL loss of the profile.
        Again, instead of calculating it \emph{a priori}, I start training with
        an initial guess and then refine the value later.
        In the original BPNet, this parameter was called $\lambda$, and this is
        the name used inside the codebase.
    \item \texttt{counts-loss-frac-target}, if supplied, turns on the adaptive
        counts loss adjustment algorithm.
        After each epoch, the \texttt{counts-loss-weight} parameter is adjusted
        so that the fraction of the loss due to counts (for this head) matches
        the specified number. See section \ref{sec:countsLossReweighting}.
    \item \begin{sloppypar}
        \texttt{output-prefix} is the file name where you want your model saved.
        For example, if you are saving models in a directory called
        \texttt{models}, and you want the model to be called \texttt{solo},
        then you'd write \verb|"output-prefix" : "models/solo"|.
        In this case, you'll find the files \texttt{models/solo.model}, which
        is the Keras model, and \texttt{models/solo.history.json}, containing
        the training history.
    \end{sloppypar}
    \item \texttt{early-stopping-patience} controls how long the network should
        wait for an improvement in the loss before quitting.
        I recommend a bit more than double the
        \texttt{learning-rate-plateau-patience}, on the order of 11.
    \item \texttt{batch-size} determines how many regions the network will look
        at simultaneously during training.
        It doesn't really matter, but if you make it too big your data won't fit
        on the GPU and if you make it too small your network will take an
        eternity to train. I like 64 or so.
    \item \texttt{learning-rate} determines how aggressive the optimizer will be
        as the network trains. 0.004 is a good bet.
        (Note that the LR will decrease during training because of the plateau
        patience.)
    \item \texttt{learning-rate-plateau-patience} controls how many epochs must
        pass without improvement before the optimizer decreases the learning
        rate.
        I recommend 5 or so.
    \item The \texttt{architecture-name} is a future-proofing argument that
        will determine what type of network you want.
        Currently, only the basic bpnet-style architecture is supported.
        See \ref{sec:modelArchitectures} for details.
    \item \texttt{input-length} is the width of the input sequence that will be
        fed into the network.
        You can use the \texttt{lengthCalc} script to calculate this based on
        a desired profile width and architecture.
    \item \texttt{output-length} is the width of the predicted profile.
        This is usually on the order of 1000.
    \item \texttt{model-name} is just a string that is stored along with the
        model.
        BPReveal does not use it internally.
    \item \texttt{model-args} is a future-proofing argument.
        If there is a new feature added to a particular architecture, the
        \texttt{model-args} string will be passed to the architecture and the
        architecture may do with that string as it pleases.
        Currently, this serves no purpose.
    \item \texttt{filters} is the number of convolutional filters at each layer.
        The more filters you add, the more patterns the model will try to learn.
        Typically this is between 32 and 128, smaller for simpler tasks.
    \item \texttt{input-filter-width} is the size of the very first
        motif-scanning layer in BPNet.
        Lately, there's been a trend of making this small, on the order of 7.
    \item \texttt{output-filter-width} is the width of the very last
        convolution, the one that actually results in the predicted profile.
        This layer is placed at the very bottom of the dilated layers.
        I use a width of 75, but many people use smaller output widths, on the
        order of 25.
    \item \texttt{max-jitter} is the maximum allowed shifting of the regions.
        This random shifting is applied during training, and helps to create
        some variety in the counts values to prevent over-fitting.
        Note that you must use the same jitter you used when you created your
        training data file - if you want to try a different jitter, you need to
        re-generate your data hdf5 files.
\end{itemize}

\newpage

\subsection{Training a transformation model: \texttt{trainTransformationModel}}\label{prog:trainTransformationModel}

The transformation input file is a JSON file that names a solo model and gives
the experimental data that it should be fit to.
Note that it may occasionally be appropriate to chain several transformation
models together.
Currently, the easiest way to do this is to feed the first transformation model
in as the solo model for the second transformation.
A better way to do it would be to write your own custom transformation Model.

\subsubsection{Input specification}
\lstinputlisting{bnf/trainTransformationModel.bnf}

This code does not support using an experimental bias track as the input to the
transformation - it must be a solo model that uses sequence as input.

\subsubsection{Parameter notes}

\begin{enumerate}
    \item \texttt{solo-model-file} is the name of the file (or directory, since
        that's how keras likes to save models) that contains the solo model.
    \item A \texttt{passthrough} transformation does nothing to the solo model,
        it doesn't regress anything.
    \item A \texttt{simple} transformation applies the specified functions to
        the output of the solo model, and adjusts the parameters to best fit the
        experimental data.
        A linear model applies $y=m x+b$ to the solo predictions (which,
        remember, are in log-space),
        a sigmoid applies $y = m_1 *sigmoid(m_2x+b_2) + b_1$,
        and a relu applies $y = m_1 * relu(m_2x+b_2) + b_1$.
        In other words, there's a linear model both before and after the sigmoid
        or relu activation.
        Generally, you need to use these more complex functions when the solo
        model is not a great fit for the experimental bias.
\end{enumerate}



\newpage

\subsection{Combined training: \texttt{trainCombinedModel}}\label{prog:trainCombinedModel}

The combined model is specified by a JSON file, much like the solo configuration
file.

\subsubsection{Input specification}

\lstinputlisting{bnf/trainCombinedModel.bnf}

\subsubsection{Parameter notes}

\begin{enumerate}
    \item \texttt{use-bias-counts} selects if you want to add the counts
        prediction from the transformation model, and the appropriateness of
        this flag will depend on the nature of your bias.
        If the bias is a constant background signal, then it makes sense to
        subtract the bias contribution to the counts.
        However, if your bias is multiplied by the underlying biology, then you
        probably shouldn't add in the bias counts since they won't affect the
        actual experiment.
    \item \texttt{input-length} refers to the input size of the \emph{residual}
        model, not the \emph{solo} model.
        The solo model, having already been created, knows its own input length.
        If the solo model's input length is smaller than the
        \texttt{input-length} setting in this config file, the sequence input
        to the solo model will automatically be cropped down to match.
\end{enumerate}




\newpage

\subsection{Prediction from bed regions: \texttt{makePredictionsBed}}\label{prog:makePredictionsBed}

This program takes in a list of regions in bed format, extracts the
corresponding sequences from a given genome file, and runs those sequences
through your model.


\subsubsection{Input specification}

\lstinputlisting{bnf/makePredictionsBed.bnf}

\subsubsection{Parameter notes}

\begin{enumerate}
    \item \texttt{heads} just gives the number of output heads for your model.
        You don't need to tell this program how many tasks there are for each
        head, since it just blindly sticks whatever the model outputs into the
        hdf5 file.
\end{enumerate}

\subsubsection{Output specification}

This program will produce an hdf5-format file containing the predicted values.
It is organized as follows:

\begin{itemize}
    \item \texttt{chrom\_names} is a list of strings that give you the meaning
        of each index in the \texttt{coords\_chrom} dataset.
        This is particularly handy when you want to make a bigwig file, since
        you can extract a header from this data.
    \item \texttt{chrom\_sizes}, the size of each chromosome in the same order
        as \texttt{chrom\_names}.
        Mostly used to create bigwig headers.
    \item \texttt{coords\_chrom} a list of integers, one for each region
        predicted, that gives the chromosome index (see \texttt{chrom\_names})
        for that region.
    \item \texttt{coords\_start}, the start base of each predicted region.
    \item \texttt{coords\_stop}, the end point of each predicted region.
    \item A subgroup for each output head of the model. The subgroups are named
        \texttt{head\_N}, where N is 0, 1, 2, etc.
        \begin{itemize}
            \item \texttt{logcounts}, a vector of shape (numRegions,) that gives
                the logcounts value for each region.
            \item \texttt{logits}, the array of logit values for each track for
                each region.
                The shape is (numRegions x outputWidth x numTasks).
                Don't forget that you must calculate the softmax on the whole
                set of logits, not on each task's logits independently.
                (There is a \texttt{logitsToProfile} function inside
                \texttt{utils.py} for doing this conversion.)
        \end{itemize}

\end{itemize}


\newpage

\subsection{Prediction from fasta: \texttt{makePredictionsFasta}}\label{prog:makePredictionsFasta}

This is the JSON file given to the prediction from fasta script.
It names the model to use and the file that contains the sequences to run
predictions on.
This program streams sequences in and writes them out as it makes predictions,
so it can be used on very large numbers of sequences without requiring much
memory.

\subsubsection{Input specification}

\lstinputlisting{bnf/makePredictionsFasta.bnf}

\subsubsection{Output specification}

This program will produce an hdf5-format file containing the description lines
from the original fasta, as well as the predicted logcounts and logits.
It is structured so:

\begin{itemize}
    \item \texttt{descriptions}, a list of strings of length (numRegions,).
        Each string corresponds to one description line (i.e., a line starting
        with \texttt{>}).
    \item A subgroup for each output head of the model.
        The subgroups are named \texttt{head\_N}, where N is 0, 1, 2, etc.
        \begin{itemize}
            \item \texttt{logcounts}, a vector of shape (numRegions,) that gives
                the logcounts value for each region.
            \item \texttt{logits}, the array of logit values for each track for
                each region.
                The shape is (numRegions x outputWidth x numTasks).
                Don't forget that you must calculate the softmax on the whole
                set of logits, not on each task's logits independently.
                (There is a \texttt{logitsToProfile} function inside
                \texttt{utils.py} that does this.)
        \end{itemize}
\end{itemize}

\newpage

\subsection{Calculating metrics: \texttt{metrics}}\label{prog:metrics}

This little helper program reads in two bigwig files and a bed of regions.
For each of the regions, it calculates several metrics, and then displays
the quintiles of the values of those metrics over the regions.
For each region, four metrics are calculated based on the profiles:
\texttt{mnll}, \texttt{jsd}, \texttt{pearsonr}, and \texttt{spearmanr}.
Then, the program displays the quintiles of the observed metrics values. The
50th percentile indicates the median value, and is likely to be the most useful
for typical work.

Additionally, this tool calculates the total counts over each region and
calculates, for all regions in the reference against all regions in the
prediction, the Pearson and Spearman correlation of the counts values.

This program uses command-line arguments rather than a configuration json,
since it doesn't do very much.
The arguments are given by running \texttt{metrics --help}.

The metrics are:
\begin{description}
    \item [mnll], the multinomial log-likelihood of seeing the observed data
        given the predicted probability distribution.
        This is the profile loss function.
    \item [jsd], the Jensen-Shannon distance.
        This technique comes from information theory, and determines how
        similar two probability distributions are.
        This is a \emph{distance} metric, so lower values indicate a better
        match.
    \item [pearsonr], the Pearson correlation coefficient of the profiles.
    \item [spearmanr], the Spearman correlation coefficient of the profiles.
    \item [Counts pearson], the Pearson correlation coefficient of total counts
        in every region.
    \item [Counts spearman], the Spearman correlation coefficient of the total
        counts in every region.
\end{description}


\newpage

\subsection{Interpret flat: \texttt{interpretFlat}}\label{prog:interpretFlat}

This is the configuration file that is passed to \progref{interpretFlat}, that
lists the regions of the genome where classical importance scores should be
calculated.
It is structured as follows:

\subsubsection{Input specification}

\lstinputlisting{bnf/interpretFlat.bnf}

\subsubsection{Parameter notes}
\begin{itemize}
    \item If you specify a \texttt{genome} and \texttt{bed-file} in the
        configuration, then this program will read coordinates from the bed
        file, extract the sequences from the provided fasta, and run
        interpretations on those sequences.
        In this case, the output file will include \texttt{chrom\_names},
        \texttt{chrom\_sizes}, \texttt{coords\_start}, \texttt{coords\_end},
        and \texttt{coords\_chrom}.
        If, however, you specify a fasta file, then the sequences are taken
        directly from that file.
        In this case, the output hdf5 file will not contain those fields, and
        instead it will contain a \texttt{descriptions} dataset, which holds
        the description lines from the fasta.
    \item The \texttt{bed-file} that you give should have regions matching the
        model's \emph{output} length. The regions will be automatically inflated
        in order to extract the input sequence from the genome.
        Somewhat confusingly, this means that the contribution scores will
        include contributions from bases that are not in your bed file.
        This is because the contribution scores explain how all of the input
        bases contribute to the output observed at the region in the bed file.
        However, if you specify \texttt{fasta-file}, then the sequences in that
        fasta must be as long as the model's \emph{input} length.
        (Since we need the whole sequence that will be explained.)
        In this case, the contribution scores in the output will match
        one-to-one with the input bases.
    \item The \texttt{heads} parameter is the \emph{total} number of heads that
        the model has, and the \texttt{head-id} parameter gives which head you
        want importance values calculated for.
    \item The \texttt{profile-task-ids} parameter lists which of the profile
        predictions (i.e., tasks) from the specified head you want considered.
        Almost always, you should include all of the profiles.
        For a single-task head, this would be \texttt{[0]}, and for a two-task
        head this would be \texttt{[0,1]}.
    \item The \texttt{profile-h5} and \texttt{counts-h5} file names are the
        output files from the algorithm.
\end{itemize}

\subsubsection{Output specification}

If you provide \texttt{bed-file} and \texttt{genome}, the output will be
structured as follows:

\begin{itemize}
    \item \texttt{chrom\_names}, a list of strings giving the name of each
        chromosome.
        \texttt{coords\_chrom} entries correspond to the order of chromosomes
        in this dataset.
    \item \texttt{chrom\_sizes}, a list of integers giving the size of each
        chromosome.
        This is mostly here as a handy reference when you want to make a bigwig
        file.
    \item \texttt{coords\_start}, the start point for each of the regions that
        were explained.
        This will have shape (num-regions,).
        Note that this starts at the beginning of the \emph{input} to the model,
        so it will not match the coordinates in the bed file.
    \item \texttt{coords\_end}, the end point for each of the regions that were
        explained.
        This will have shape (num-regions,).
        As with \texttt{coords\_start}, this corresponds to the last base in the
        \emph{input} to the model.
    \item \texttt{coords\_chrom}, the chromosome number on which each region is
        found.
        These are integer indexes into \texttt{chrom\_names}, and this dataset
        has shape (num-regions,)
    \item \texttt{input\_seqs}, a one-hot encoded array representing the input
        sequences.
        It will have shape (num-regions x input-length x 4)
    \item \texttt{hyp\_scores}, a table of the shap scores.
        It will have shape (num-regions x input-length x 4).
        If you want the actual contribution scores, not the hypothetical ones,
        multiply \texttt{hyp\_scores} by \texttt{input\_seqs} to zero out
        all purely hypothetical contribution scores.

\end{itemize}

However, if you just provide a fasta of sequences to analyze, then it will be
structured so:

\begin{itemize}
    \item \texttt{descriptions}, a list of strings that are the description
        lines from the input fasta file (with the leading \texttt{>} removed).
        This list will have shape (num-regions,)
    \item \texttt{input\_seqs} and \texttt{hyp\_scores}, with the same meaning
        as in the bed-and-genome based output files.
\end{itemize}

Note that while you can use \progref{shapToNumpy} on either format of
\progref{interpretFlat} output, you cannot convert a fasta-based interpretation
h5 to a bigwig, since it doesn't contain coordinate information.


\newpage
\subsection{Interpret pairwise importances with PISA: \texttt{interpretPisa}}\label{prog:interpretPisa}

This is the JSON file given to the PISA interpretation tool. PISA is described
in \ref{sec:pisa}.
If you provide a bed file, but this file represents the *individual bases* that
should be shapped.
There is no restriction on the number of regions, nor on their length.
If you are interested in the effects of a particular motif, then you'd put the
region surrounding that motif in the bed file, making it as large as you want
to see the interactions you're interested in

You can also supply the sequences directly with a fasta file.
Since PISA calculates shap scores for a single base, this tool always calculates
the pisa scores for the \emph{leftmost} base in the output window.
PISA is described in \ref{sec:pisa}.

Suppose the input length is 3090 bp, and the output is 1000 bp.
In this case, the receptive field is 2091 bp, and there are 1045 bp of overhang
on each end of the input sequence.
So, for each input sequence, this program will assign shap scores to the 1046th
base (one-indexed) of the input sequence.
.

\subsubsection{Input specification}

\lstinputlisting{bnf/interpretPisa.bnf}

\subsubsection{Output specification}

It produces an hdf5 format which is organized as follows:

\begin{itemize}
    \item \texttt{head-id}, an integer representing which head of the model was
        used to generate the data.
    \item \texttt{task-id}, an integer giving the task number within the
        specified head.
    \item \texttt{input\_predictions}, a (numSamples,) array of the logit value
        of the target base when that sequence is run through the network.
    \item \texttt{shuffle\_predictions}, a (numSamples, numShuffles) array of
        the logits of the target base in the shuffled reference sequences.
    \item \texttt{sequence}, a one-hot encoded array representing the sequence
        under each PISA value.
        The shape is (num regions * receptive-field * 4).
        Note that this is receptive field, not input width, since each base
        being shapped will only be affected by bases in its receptive field,
        and there's no reason to store the noise.
    \item \texttt{shap}, a table of the shap scores.
        The shape is the same as the sequence table, and each position in the
        shap table represents the corresponding base in the sequence table.
        These values are contribution scores to the difference-from-reference
        of the logit at this base.

\end{itemize}

If you specified a bed file with input regions, it will also have these datasets:

\begin{itemize}
    \item \texttt{chrom\_names}, a list of strings giving the name of each
        chromosome.
        This is used to figure out which chromosome each number in
        \texttt{coords\_chrom} corresponds to.
    \item \texttt{chrom\_sizes}, a list of integers giving the size of each
        chromosome.
        This is mostly here as a handy reference when you want to make a bigwig
        file.
    \item \texttt{coords\_base}, the center point for each of the regions in the
        table of PISA values.
    \item \texttt{coords\_chrom}, the chromosome on which each PISA vector is
        found.
        This is a list of integers.
        The width of the integer data type may vary from run to run, and is
        calculated based on the number of chromosomes in the genome file.
\end{itemize}

If, however, you gave a fasta file of input sequences, it will instead have the
following dataset:

\begin{itemize}
    \item \texttt{descriptions}, a string taken from the fasta file.
        These are the comment (\texttt{>}) lines, and are stored without the
        leading \texttt{>}.
        This will have shape (numSamples,)

\end{itemize}

\newpage

\subsection{Preparing bed files: \texttt{prepareBed}}\label{prog:prepareBed}

This is the JSON file given to the input preparation script.
It splits your regions into test, train, and validation regions, and optionally
applies some filtering.
N.B. This program does not validate the name, score, or strand columns of the
bed file.
They are retained exactly as passed in.

\subsubsection{Input specification}

\lstinputlisting{bnf/prepareBed.bnf}

\subsubsection{Parameter notes}

\begin{itemize}
    \item The \texttt{resize-mode} specifies where in the regions in the bed
        file the output regions should be centered.
        Note that this program assumes your bed files are in bed3 format, that
        is, (chrom, start, stop).
        If you have additional columns with information like peak offset, those
        data will be ignored.
    \item The max and min quantile values, if provided, will be used to
        threshold which regions are included in the output.
        First, all of the counts in the given regions are computed (which takes
        a while!), and then the given quantile is computed.
        All regions exceeding that value are not included in the output files.
    \item Similarly, if max and min counts are given, all regions having more
        (or fewer) reads than the given number will be excluded.
    \item If you use regexes to make your splits, then the \texttt{name} entry
        of the bed files will be compared against each regex.
        On a match, that region will be included in that split.
    \item The \texttt{remove-overlaps} flag can be set to \texttt{true} if you'd
        like to exclude overlapping regions.
        This is done by resizing all regions down to
        \texttt{overlap-max-distance}, and then, if multiple regions have an
        overlap, one is deleted at random.
        If \texttt{remove-overlaps} is \texttt{false}, then
        \texttt{overlap-max-distance} does not need to be set.
    \item The old \texttt{bigwigs} format is deprecated and will be removed
        in \deprecation{BPReveal 5.0.0}\label{dep:oldbigwigs}
\end{itemize}

I should mention that the maximum and minimum counts are not compared across the
same window.
When comparing a region against the maximum counts value, all counts within the
$input\mbox{-}length + jitter$ are added up.
This way, if you have a crazy-huge spike just outside your region, that region
will be rejected.
Conversely, for minimum counts, the counts within the
$output\mbox{-}length - jitter$ will be considered.
This way, no matter what jitter value is selected, there will be at least the
given number of counts in the region.


\newpage

\subsection{Modisco seqlet analysis: \texttt{motifSeqletCutoffs}}\label{prog:motifSeqletCutoffs}
In order to see where seqlets are found on the genome, we need to scan the cwms
derived from modiscolite.
The first step of this process is to look at the seqlets that MoDISco called for
each pattern it identified, and establish cutoff values.
This program does that.
It reads in a modiscolite hdf5 file and calculates cutoff values of seqlet
similarity for what constitutes a hit.
It requires one JSON-format input, and generates two outputs: First, it
generates a tsv file containing all of the seqlets in the modisco h5 with some
helpful metadata, like how well they match the pattern they are identified
as being a part of.
Second, it produces a JSON-format file that will be needed
by \progref{motifScan}.

\subsubsection{Input specification}
\lstinputlisting{bnf/motifSeqletCutoffs.bnf}

\subsubsection{Parameter notes}
\begin{itemize}
    \item \texttt{seqlets-tsv}, if provided, is the name of the file that should
        be written containing the scanned seqlets.
        See \progref{motifAddQuantiles} for the structure of this file.
    \item \texttt{modisco-contrib-h5}, if provided, gives the contribution score
        file generated by \texttt{interpretFlat}, which is necessary to recover
        the genomic coordinates of the seqlets, since the Modisco hdf5 doesn't
        contain that info.
        The contribution scores are *not* extracted from this file, just
        coordinates.
        \emph{THIS DOES NOT CURRENTLY WORK, SINCE SEQLET INDEXES ARE RESET BY
        MODISCO}
    \item There are two ways of specifying patterns, either by giving each
        pattern and metacluster pair individually, or by listing multiple
        patterns under a single metacluster.
        The short-names, if provided, will be used to populate the name field in
        the generated tsv.
        You could use this to give a particular pattern the name of its binding
        protein.
    \item TODO ADD DOCUMENTATION FOR QUANTILE AND TRIM PARAMETERS.
    \item The background-probs field gives the genetic content of your genome.
        For example, if you had a genome with 60 percent GC content, this would
        be [0.2, 0.3, 0.3, 0.2].
        The order of the bases is A, C, G, and T.
\end{itemize}

\subsubsection{Output specification}
See \texttt{motifAddQuantiles.py} for a description of the tsv file,
and \texttt{motifScan.py} for a description of the generated JSON.


\newpage


\subsection{Scanning for motifs: \texttt{motifScan}}\label{prog:motifScan}

This program scans over the contribution scores you calculated with
\progref{interpretFlat} and looks for matches to motifs called by modiscolite.
It can be run with a quantile JSON from \progref{motifSeqletCutoffs}, or you can
include the settings for that program inside the JSON for this one, in which
case it will perform the seqlet analysis first and save those results for you.
If you include a \texttt{seqlet-cutoff-settings} block in the config, it will
run the \progref{motifSeqletCutoffs} tools, and if you don't include that, you
must include a \texttt{seqlet-cutoff-json} file with the appropriate cutoffs.
It is an error to specify both \texttt{seqlet-cutoff-settings} and
\texttt{seqlet-cutoff-json}.


\subsubsection{Input specification}

\lstinputlisting{bnf/motifScan.bnf}

You also need a JSON file containing the information for each pattern.
This file is generated by \progref{motifSeqletCutoffs} and saved to the name
\texttt{quantile-json} in the configuration to that script.

\begin{lstlisting}
<quantile-json> ::=
    [<list-of-scan-patterns> ]

<list-of-scan-patterns> ::=
    <scan-pattern>
  | <scan-pattern>, <list-of-scan-pattern>

<scan-pattern> ::=
    {"metacluster-name" : <string>,
     "pattern-name" : <string>,
     "short-name" : <string>
     "cwm" : <motif-array>,
     "pssm" : <motif-array>,
     "seq-match-cutoff" : <float-or-null>,
     "contrib-match-cutoff" : <float-or-null>,
     "contrib-magnitude-cutoff" : <float-or-null>}

<motif-array> ::=
    [ <list-of-base-arrays> ]

<list-of-base-arrays> ::=
    <base-array>
  | <base-array>, <list-of-base-arrays>

base-array ::=
    [<float>, <float>, <float>, <float>]
\end{lstlisting}


\subsubsection{Parameter notes}
\begin{itemize}
    \item The \texttt{scan-contrib-h5} file is the output of
        \progref{interpretFlat} and contains contribution scores.
        All of the regions in this file will be scanned.
    \item \texttt{num-threads} is the number of parallel workers to use.
        Due to the streaming architecture of this program, the minimum value
        of \texttt{num-threads} is 3.
        I have found that this program scales very well up to 70 cores, and
        haven't tested it beyond that.
\end{itemize}

In the quantile JSON, we find the actual numerical cutoffs for scanning.

\begin{itemize}
    \item \texttt{metacluster-name} and \texttt{pattern-name} are from the
        modisco hdf5 file.
    \item \texttt{short-name} is a convenient name for this motif, and is
        entirely up to you.
        The short name will be used to populate the name column in the generated
        bed and csv files.
    \item \texttt{cwm} is an array of shape (length, 4) that contains the cwm of
        the motif.
        It is used to calculate the Jaccard similarity and the L1 score.
    \item \texttt{pssm} is the sequence-based information content at each
        position, and is used to calculate sequence match scores.
    \item The three cutoff values are the actual scores,
        \emph{not quantile values}.
        These are calculated by \\ \progref{motifSeqletCutoffs}.
        You could set these manually, but why would you?
\end{itemize}

\subsubsection{Output specification}
For the generated tsv file, see \progref{motifAddQuantiles}.
If you include a \texttt{quantile-json} in
your \\ \texttt{seqlet-cutoff-settings}, then running \progref{motifScan} will
save out the cutoff JSON.


\newpage


\subsection{Annotating seqlets with quantile information: {motifAddQuantiles}}\label{prog:motifAddQuantiles}

This little helper program calculates quantile values for seqlets and called
motif instances.
For each pattern (patterns in different metaclusters are distinct), it looks at
the seqlets and determines where that seqlet's importance magnitude,
contribution match, and sequence match scores fall among other seqlets in that
pattern.
Then, for motif hits, it sees where each hit falls, in terms of quantile, among
the seqlets in that pattern.

The quantile is based on a very simple definition.
A particular seqlet's quantile is calculated by sorting all of the seqlets for
one pattern.
The lowest-scoring seqlet gets a quantile of 0.0, the highest-scoring gets 1.0,
and the seqlets in between get quantile values based on their order in the
sorted metric.

For scanned hits, we take the sorted array of seqlet statistics (for the same
pattern as the matched hit fell into) and ask, 'where would the score of this
hit rank among the sorted array of seqlet scores?'
The hit's rank is then its quantile score.
If a hit falls between the scores of two seqlets, then a linear interpolation
is performed to assign a quantile value.

The input and ouput to this program are tsv files, and the only difference in
format is that the outputs have three additional columns:
\texttt{contrib\_magnitude\_quantile},\\ \texttt{seq\_match\_quantile}, and
\texttt{contrib\_match\_quantile}.
The remaining columns are described below:

\begin{itemize}
    \item \texttt{chrom} gives the chromosome where the seqlet or hit was found.
    \item \texttt{start} gives the start position (inclusive, zero-based) of the
        seqlet or motif hit.
    \item \texttt{end} gives the end position (exclusive, zero-based) of the
        seqlet or motif hit.
    \item \begin{sloppypar}\texttt{short\_name} is the user-provided name for
            this motif.
            If you didn't provide one in the configuration for
            \progref{motifSeqletCutoffs}, then it will be something like
            \texttt{pos\_0} for the positive metacluster, pattern zero.
        \end{sloppypar}
    \item \texttt{contrib\_magnitude} is the total contribution across this
        motif instance.
        A higher value means more motif contribution.
    \item \texttt{strand} is a single character indicating if the motif was on
        the positive or negative strand.
    \item \texttt{metacluster\_name} is straigh from the modisco hdf5 file.
        It will be something like \texttt{pos\_patterns}.
    \item \texttt{pattern\_name} is also from the modisco hdf5.
        It will be something like \texttt{pattern\_5}.
    \item \texttt{sequence} is the DNA sequence of that motif instance.
    \item \texttt{index} gives either the region index in the contribution hdf5
        (from \progref{motifScan}), or the seqlet index in the modisco hdf5
        (from \progref{motifSeqletCutoffs}).
    \item \texttt{seq\_match} gives the information content of the sequence
        match to the motif's pwm.
    \item \texttt{contrib\_match} gives the continuous Jaccard similarity
        between the motif's cwm and the contribution scores of this seqlet.
\end{itemize}


If a contribution hdf5 file was not provided to \progref{motifSeqletCutoffs},
the chrom, start, and end columns are meaningless.
Note that the first six columns define a bed file, and a simple \texttt{cut}
command can generate a viewable bed file from these tsvs:

\lstinputlisting{bnf/motifAddQuantiles.bnf}

Note that the hits from scanning can contain duplicates.
This can happen if the same bases appear in multiple regions (i.e., there is
overlap in the region set).
In this case, it makes sense to only keep the best instance (highest importance
magnitude) of that motif hit.
This can be done with a little Unix-fu:

\begin{lstlisting}
    cat scan.tsv | \
        cut -f 1-6 | \
        tail -n +2 | \
        sort -k1,1 -k2,2n -k3,3n -k4,4 -k5,5nr | \
        awk '!_[$1,$2,$3,$4,$6]++' > scan.bed
\end{lstlisting}
%stopzone
\newpage
\section{Model architectures}\label{sec:modelArchitectures}

The precise details of the model architectures can be found in models.py, but
they share some common themes.
Every model that ever gets saved to disk accepts a one-hot encoded sequence as
input, and produces outputs that are grouped into ``heads''.
A model may generate any number of heads, and the heads may have different
sizes.
In general, each head should represent one set of DNA fragments.
For example, an experiment that produces cut sites on the + and - strand of DNA
produces two tracks, but the tracks represent two ends of the same fragments.
So these two tracks would be in the same head.
However, if you have an experiment where it's appropriate to split fragments into
``short'' (100-500 bp) and ``long'' (1 kb to 10 kb), then those tracks do not
represent the same fragments, so they should be in different heads.

If you have done ChIP-nexus on three different factors, then you'd have three
heads, each one corresponding to a different factor, and each head would predict
both the + and - strand data for that factor.

If you're not sure if you can combine your data under one output head, it's much
safer to split the data into multiple heads.

A head contains a profile prediction and a counts prediction.
The profile prediction is a tensor of shape
(batch-size x) number-of-tracks x output-width, and each value in this tensor
is a logit.
Note that the \emph{whole} profile prediction should be considered when taking
the softmax.
That is to say, the profile of the first track is NOT
$e^{logcounts} * softmax(profile_{0,:})$, but rather you have to take the
softmax first and then slice out the profile:
$e^{logcounts} * softmax(profile)_{0,:}$.
There is a function, \texttt{logitsToProfile}, in \texttt{utils.py} that does
this automatically.

Of course, if the profile only has one track, this distinction is vacuous.
The counts output is a scalar that represents the natural logarithm of the
number of reads predicted for the current region.

It is possible to add more model architectures, but currently the program only
supports a BPNet-style architecture.
You can take a look at soloModel in \texttt{models.py} for details on how it
works.

\subsection{Adaptive counts loss reweighting}\label{sec:countsLossReweighting}
In old BPNet, you had to specify a counts loss weight that was used during
training.
This parameter adjusts how much of the loss is due to the counts component
(mean-squared-error of logcounts) and how much is due to the profile
(multinomial log likelihood).
This is great, but you don't know beforehand what exact number you should apply
to the counts loss in order to get your desired fraction.
As an equation,

\begin{equation}
    f \equiv \frac{\lambda c}{\lambda c + p}
\end{equation}

Where $f$ is the fraction of loss due to counts, $c$ is the raw counts loss,
$p$ is the raw profile loss, and $\lambda$ is the \texttt{counts-loss-weight}
parameter.
If you want, say, ten percent of your loss to come from counts, you need to pick
a value of $\lambda$ so that that $f = 0.1$.
But since we don't know the values of $c$ and $p$ that we'll get during
training, we have to guess.

The adaptive counts loss algorithm skirts this issue by updating $\lambda$
during training to match your desired $f$.
Starting with the value of \texttt{counts-loss-weight} you specify in the
configuration file, the algorithm springs into action at the end of each epoch.

\begin{align}
    \lambda^\prime_{E+1} &= \frac{p f_{target}}{(1-f_{target}) c} \\
    \lambda^{\prime\prime}_{E+1} &= \beta \lambda^\prime_{E+1} + (1 - \beta) \lambda_{E} \\
    \gamma &\equiv \frac{\lambda^{\prime\prime}_{E+1}}{\lambda_E} \\
    \lambda_{E+1} &= 
    \begin{cases}
        2 \lambda_E & \gamma > 2 \\
        \lambda^{\prime\prime}_{E+1} & \frac{1}{2} \le \gamma \le 2 \\
        \frac{\lambda_E}{2} & \gamma < \frac{1}{2}
    \end{cases}
\end{align}

where $\lambda_{E+1}$ is the $\lambda$ value for the next epoch, $\lambda_E$
is the current $\lambda$, $\lambda^{\prime}$ is the value of $\lambda$ which,
given the current profile loss $p$ and counts loss $c$, would give the fraction
target $f_{target}$.
($f_{target}$ should be between 0 (only care about profile) and 1 (only care
about counts), exclusive.
A normal value would be something like 0.1.)
$\lambda^{\prime\prime}$ is a smoothed version of $\lambda$, with aggression
parameter $\beta$. $\beta=0$ implies no change ever to $\lambda$, while
$\beta = 1$ means that the $\lambda$ from the last epoch is ignored.
The scripts in bpreveal use a $\beta$ parameter of 0.3. This is currently not
user-configurable, but can be edited in \texttt{callbacks.py}.

Early in training, $\lambda_E$ may be \emph{way} off from
$\lambda^{\prime\prime}_{E+1}$, and so we clamp $\lambda_{E+1}$ to be at most
a factor of two off from $\lambda_E$.

\section{PISA}\label{sec:pisa}
The traditional method of interpretation of BPNet models asks the question,
``How is it that this base here affects some readout over this window?'',
where the readout is a single scalar value calculated over an entire prediction
window.
If our readout is counts, then we are calculating how much each input base
contributed to the total counts over our prediction window.
For profile, we calculate a weighted mean-normalized logit value (as was done in
the original BPNet paper).
In this way, each base gets a single scalar value for how it contributed to the
overall readout.

PISA asks a subtly different question:
``How is it that this base here affects the readout at that base over there?''
In this case, instead of looking at how much one base contributes to a global
readout, we're asking about its effect on a single one of its neighbors.
With PISA, each base gets a vector of its contributions to the readout at each
one of its neighbors.
This vector has shape (receptive-field,)
Accordingly, if we perform PISA on a region of the genome, we would get a PISA
array of shape (region-length, receptive-field).

We use code derived from \texttt{deepshap} to perform this calculation.
With deepShap, we create an explainer that looks at one output from the model.
For the sake of simplicity, all of the PISA implementation looks at the leftmost
base in the output window, but this implementation detail has no effect on the
actual calculated values.
Once the explainer has been created, we provide it with genomic sequences to
explain, and it assigns, to each base in the input, its contribution to the
observed output.

The outputs of the model are logits, and the contribution scores have the same
units, so the explainer is effectively assigning a (base $e$) fold-change value
to each input.

There are several properties of Shapley values that are important here.
In these formulae, $\phi_i$ is the contribution score of base $i$, drawn from
the sequence $S$.
The readout that we're measuring, the logit at the leftmost base, is $v(S)$.
I'll also use $K$ to refer to a subset of the input sequence $S$.
I'll use $R$ to refer to an ensemble of reference sequences, and the average
prediction from those reference sequences is $\bar{v}(R)$

The first, and arguably most important, property of Shapley values is
\emph{efficiency}:

\begin{equation}
    \sum_{i \in S} \phi_i(v) = v(S) - \bar{v}(R)
\end{equation}

This means that if we add up all the shap values that were assigned for a
particular logit, we recover the difference-from-reference of that logit.

One possible weakness of a method like this has to do with cooperation.
Suppose that some readout is observed if at least one of the bases in a region
is A.
If two bases are A, we observe that readout.
But how should we assign $\phi$ values to those two bases?
With shapley values, we are guaranteed that they will get the same score:

\begin{equation}
    \Bigl( \forall (K \subseteq S \backslash \{i,j\})\;
    \bigl(v(K \cup \{i\}) = v(K \cup \{j\})\bigr) \Bigr)
    \\
    \implies \phi_i(v) = \phi_j(v)
\end{equation}

The third property turns out to be very important in performing PISA on
bias-corrected models.
The combined model uses a simple sum to combine the logits from the solo model
and the residual model, so $v_{combined}(S) = v_{solo}(S) + v_{residual}(S)$.
Shapley values preserve this linearity, ensuring that it's meaningful to look
at PISA plots of a residual model:

\begin{equation}
    \phi_i(u + v) = \phi_i(u) + \phi_i(v)
\end{equation}

Finally, an almost-obvious property: If a particular base has no effect on the
readout, its $\phi$ values should be zero. So it is:

\begin{equation}
    \Bigl( \forall (K \subseteq S \backslash \{i\})
    \bigl( v(K \cup \{i\}) = v(K)\bigr)\Bigr)
    \\
    \implies phi_i(v) = 0
\end{equation}

\end{document}

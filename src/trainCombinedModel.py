#!/usr/bin/env python3
"""Trains up a residual model to remove an uninteresting signal from an experiment.

BNF
---

.. highlight:: none

.. literalinclude:: ../../doc/bnf/trainCombinedModel.bnf


Parameter Notes
---------------
Most of the parameters for the combined model are the same as for a solo
model, and they are described at
:py:mod:`trainSoloModel<bpreveal.trainSoloModel>`.

use-bias-counts
    Selects if you want to add the counts prediction from the transformation
    model, and the appropriateness of this flag will depend on the nature of
    your bias. If the bias is a constant background signal, then it makes sense
    to subtract the bias contribution to the counts. However, if your bias is
    multiplied by the underlying biology, then you probably shouldn't add in
    the bias counts since they won't affect the actual experiment.

transformation-model-file
    The name of the Keras model file generated by
    :py:mod:`trainTransformationModel<bpreveal.trainTransformationModel>`.

input-length
    The input size of the *residual* model, not the *solo* model. The solo
    model, having already been created, knows its own input length. If the solo
    model's input length is smaller than the ``input-length`` setting in this
    config file, the sequence input to the solo model will automatically be
    cropped down to match.

HISTORY
-------

Before BPReveal 3.0.0, the solo model had to have the same input length as
the residual model. An auto-cropdown feature was implemented by Melanie Weilert
to remove this restriction.

API
---
"""
import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "1"
import json
from bpreveal import utils
if __name__ == "__main__":
    utils.setMemoryGrowth()
import h5py
from tensorflow import keras
from bpreveal import generators
from bpreveal import losses
from bpreveal.callbacks import getCallbacks
from bpreveal import models
from bpreveal import logUtils
import tensorflow as tf


def trainModel(model, inputLength, outputLength, trainBatchGen,  # pylint: disable=unused-argument
               valBatchGen, epochs, earlyStop, outputPrefix,
               plateauPatience, heads):
    """Train the model."""
    callbacks = getCallbacks(earlyStop, outputPrefix, plateauPatience, heads)
    if logUtils.getLogger().isEnabledFor(logUtils.INFO):
        verbosity = "auto"
    else:
        verbosity = 0
    history = model.fit(trainBatchGen, epochs=epochs, validation_data=valBatchGen,
                        callbacks=callbacks, max_queue_size=1000,
                        verbose=verbosity)
    # Turn the learning rates into native python float values so they can be saved to json.
    history.history["lr"] = [float(x) for x in history.history["lr"]]
    lossCallback = callbacks[3]
    history.history["counts-loss-weight"] = lossCallback.λHistory
    return history


def main(config):
    """Build and train a combined model."""
    logUtils.setVerbosity(config["verbosity"])
    inputLength = config["settings"]["architecture"]["input-length"]
    outputLength = config["settings"]["architecture"]["output-length"]
    numHeads = len(config["heads"])
    regressionModel = utils.loadModel(
        config["settings"]["transformation-model"]["transformation-model-file"])
    regressionModel.trainable = False
    logUtils.debug("Loaded regression model.")
    combinedModel, residualModel, _ = models.combinedModel(
        inputLength, outputLength,
        config["settings"]["architecture"]["filters"],
        config["settings"]["architecture"]["layers"],
        config["settings"]["architecture"]["input-filter-width"],
        config["settings"]["architecture"]["output-filter-width"],
        config["heads"], regressionModel)
    logUtils.debug("Created combined model.")
    profileLosses = [losses.multinomialNll] * numHeads
    countsLosses = []
    profileWeights = []
    countsWeights = []
    for head in config["heads"]:
        profileWeights.append(head["profile-loss-weight"])
        # For adaptive loss weights, make the counts loss a keras variable so I
        # can update it during training.
        λInit = head["counts-loss-weight"] if "counts-loss-weight" in head else 1
        λ = tf.Variable(λInit, dtype=tf.float32)
        # Store the (keras) variable with the loss weight in the head dictionary.
        # We'll need it in the callbacks, and this is a reasonable place to store it.
        head["INTERNAL_λ-variable"] = λ
        countsWeights.append(1)
        countsLosses.append(losses.weightedMse(λ))

    residualModel.compile(
        optimizer=keras.optimizers.Adam(learning_rate=config["settings"]["learning-rate"]),
        jit_compile=True,
        loss=profileLosses + countsLosses,
        loss_weights=profileWeights + countsWeights)  # + is list concatenation, not addition!

    combinedModel.compile(
        optimizer=keras.optimizers.Adam(learning_rate=config["settings"]["learning-rate"]),
        jit_compile=True,
        loss=profileLosses + countsLosses,
        loss_weights=profileWeights + countsWeights)  # + is list concatenation, not addition!
    logUtils.debug("Models compiled.")
    trainH5 = h5py.File(config["train-data"], "r")
    valH5 = h5py.File(config["val-data"], "r")

    trainGenerator = generators.H5BatchGenerator(config["heads"], trainH5,
                                                 inputLength, outputLength,
                                                 config["settings"]["max-jitter"],
                                                 config["settings"]["batch-size"])
    valGenerator = generators.H5BatchGenerator(config["heads"], valH5,
                                               inputLength, outputLength,
                                               config["settings"]["max-jitter"],
                                               config["settings"]["batch-size"])
    logUtils.info("Generators initialized. Training.")

    history = trainModel(combinedModel, inputLength, outputLength, trainGenerator,
                         valGenerator, config["settings"]["epochs"],
                         config["settings"]["early-stopping-patience"],
                         config["settings"]["output-prefix"],
                         config["settings"]["learning-rate-plateau-patience"],
                         config["heads"])
    combinedModel.save(config["settings"]["output-prefix"] + "_combined" + ".model")
    residualModel.save(config["settings"]["output-prefix"] + "_residual" + ".model")
    with open("{0:s}.history.json".format(config["settings"]["output-prefix"]), "w") as fp:
        json.dump(history.history, fp, ensure_ascii=False, indent=4)


if __name__ == "__main__":
    import sys
    with open(sys.argv[1], "r") as configFp:
        configJson = json.load(configFp)
    import bpreveal.schema
    bpreveal.schema.trainCombinedModel.validate(configJson)
    main(configJson)
